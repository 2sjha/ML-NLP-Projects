{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQAJtbwk6J2K",
        "outputId": "8a50d19f-4d18-433e-d267-d1d2da690331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X53R4nxZeW6dulZLxu7lCefpAX6V-jFG\n",
            "To: /content/netflix.zip\n",
            "\r  0% 0.00/15.9M [00:00<?, ?B/s]\r100% 15.9M/15.9M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1X53R4nxZeW6dulZLxu7lCefpAX6V-jFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8D9mZMAz4e2K"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implements Collaborative Filtering Algoirithm\n",
        "\"\"\"\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "\n",
        "class CollabFilter:\n",
        "    \"\"\"\n",
        "    Implements Collaborative Filtering Algoirithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        self.mean_user_ratings = defaultdict(float)\n",
        "        self.movie_rated_users = defaultdict()\n",
        "\n",
        "        self.train(dataset)\n",
        "        self.test(dataset)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Calculates mean votes per user and creates a map of movie to users who have rated that movie\n",
        "        \"\"\"\n",
        "        # Init empty list of users for each movie\n",
        "        for movie in dataset[\"movies\"].keys():\n",
        "            self.movie_rated_users[movie] = set()\n",
        "\n",
        "        for user_id, ratings in dataset[\"ratings\"][\"train\"].items():\n",
        "            ratings_count = len(ratings)\n",
        "            self.mean_user_ratings[user_id] = float(0.0)\n",
        "            for mv_id, mv_rating in ratings.items():\n",
        "                # Add user to list of users for this movie mv_id\n",
        "                self.movie_rated_users[mv_id].add(user_id)\n",
        "                # Add this movie's rating for mean rating\n",
        "                self.mean_user_ratings[user_id] += mv_rating\n",
        "            # Divide by count of ratings to calculate mean rating\n",
        "            self.mean_user_ratings[user_id] /= ratings_count\n",
        "\n",
        "    def test(self, dataset):\n",
        "        \"\"\"\n",
        "        Calculates error between predicted ratings and actual ratings\n",
        "        \"\"\"\n",
        "        mean_absolute_error = float(0.0)\n",
        "        root_mean_squared_error = float(0.0)\n",
        "        count = 0\n",
        "\n",
        "        # Calculate weights to access them later, instead of\n",
        "        # calculating it at run time for every test instance\n",
        "        test_users = defaultdict()\n",
        "        for user_id, user_ratings in dataset[\"ratings\"][\"test\"].items():\n",
        "            test_users[user_id] = user_ratings.keys()\n",
        "\n",
        "        self.calculate_weights_for_users(dataset, test_users)\n",
        "\n",
        "        print(\"actual_rating\\tpredicted_rating\")\n",
        "        print(\"------------------------------------------------------------------\")\n",
        "        for user_id, user_ratings in dataset[\"ratings\"][\"test\"].items():\n",
        "            for mv_id, actual_rating in user_ratings.items():\n",
        "                predicted_rating = self.predict_user_mv_rating(dataset, user_id, mv_id)\n",
        "                print(str(actual_rating) + \"\\t\\t\\t\\t\" + str(predicted_rating))\n",
        "\n",
        "                mean_absolute_error += abs(predicted_rating - actual_rating)\n",
        "                root_mean_squared_error += pow(abs(predicted_rating - actual_rating), 2)\n",
        "                count += 1\n",
        "        print(\"-------------------------------------------------\\n\")\n",
        "\n",
        "        mean_absolute_error /= count\n",
        "        root_mean_squared_error /= count\n",
        "        root_mean_squared_error = pow(root_mean_squared_error, 0.5)\n",
        "        print(\"Mean absolute error: \" + str(mean_absolute_error))\n",
        "        print(\"Root mean squared error: \" + str(root_mean_squared_error))\n",
        "\n",
        "    def calculate_weight_for_two_users(self, dataset, user_i, user_j) -> float:\n",
        "        \"\"\"\n",
        "        Finds common movies rated by user_i and user_j,\n",
        "        Uses it to calculate weight for user_i and user_j\n",
        "        \"\"\"\n",
        "        common_rated_movies = set(dataset[\"ratings\"][\"train\"][user_i].keys()) & set(\n",
        "            dataset[\"ratings\"][\"train\"][user_j].keys()\n",
        "        )\n",
        "        if len(common_rated_movies) == 0:\n",
        "            return float(0.0)\n",
        "        else:\n",
        "            numerator = float(0.0)\n",
        "            denominator1 = float(0.0)\n",
        "            denominator2 = float(0.0)\n",
        "            for mv_id in common_rated_movies:\n",
        "                numerator += (\n",
        "                    dataset[\"ratings\"][\"train\"][user_i][mv_id]\n",
        "                    - self.mean_user_ratings[user_i]\n",
        "                ) * (\n",
        "                    dataset[\"ratings\"][\"train\"][user_j][mv_id]\n",
        "                    - self.mean_user_ratings[user_j]\n",
        "                )\n",
        "\n",
        "                denominator1 += pow(\n",
        "                    (\n",
        "                        dataset[\"ratings\"][\"train\"][user_i][mv_id]\n",
        "                        - self.mean_user_ratings[user_i]\n",
        "                    ),\n",
        "                    2,\n",
        "                )\n",
        "                denominator2 += pow(\n",
        "                    (\n",
        "                        dataset[\"ratings\"][\"train\"][user_j][mv_id]\n",
        "                        - self.mean_user_ratings[user_j]\n",
        "                    ),\n",
        "                    2,\n",
        "                )\n",
        "\n",
        "            if denominator1 == 0 or denominator2 == 0:\n",
        "                return float(0.0)\n",
        "            else:\n",
        "                return float(numerator / pow(denominator1 * denominator2, 0.5))\n",
        "\n",
        "    def calculate_weights_for_users(self, dataset, test_users):\n",
        "        \"\"\"\n",
        "        calculates rating weights for all test users using equation 2\n",
        "        \"\"\"\n",
        "        for user_id, rated_movies in test_users.items():\n",
        "            wt_users = set()\n",
        "            user_wt = defaultdict()\n",
        "            for mv_id in rated_movies:\n",
        "                for mv_user in self.movie_rated_users[mv_id]:\n",
        "                    if mv_user != user_id:\n",
        "                        wt_users.add(mv_user)\n",
        "\n",
        "            for mv_user in wt_users:\n",
        "                wt = self.calculate_weight_for_two_users(dataset, user_id, mv_user)\n",
        "                user_wt[mv_user] = wt\n",
        "\n",
        "            # Normalize the weights\n",
        "            wt_sum = float(0.0)\n",
        "            for user_j_wt in user_wt.values():\n",
        "                wt_sum += user_j_wt\n",
        "\n",
        "            if wt_sum != 0:\n",
        "                for user_j in user_wt:\n",
        "                    user_wt[user_j] /= wt_sum\n",
        "\n",
        "            # Writing weights to disk since I encountered a Memory error\n",
        "            with open(\"./weights/\" + user_id + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(user_wt, f)\n",
        "\n",
        "    def predict_user_mv_rating(self, dataset, active_user_id, mv_id):\n",
        "        \"\"\"\n",
        "        predicts user vote for a movie using equation 1\n",
        "        \"\"\"\n",
        "        # Start with mean rating for active user\n",
        "        predicted_rating = self.mean_user_ratings[active_user_id]\n",
        "\n",
        "        # Load the weights from disk\n",
        "        with open(\n",
        "            \"./weights/\" + active_user_id + \".json\", \"r\", encoding=\"utf-8\"\n",
        "        ) as user_wt_json:\n",
        "            weights = json.load(user_wt_json)\n",
        "            # Calculate weighted sum of other users' ratings\n",
        "            for curr_user_i, curr_user_wt in weights.items():\n",
        "                # Only count rating if this curr_user has rated movie mv_id\n",
        "                if (\n",
        "                    mv_id in dataset[\"ratings\"][\"train\"][curr_user_i]\n",
        "                    and curr_user_wt != 0\n",
        "                ):\n",
        "                    predicted_rating += curr_user_wt * (\n",
        "                        dataset[\"ratings\"][\"train\"][curr_user_i][mv_id]\n",
        "                        - self.mean_user_ratings[curr_user_i]\n",
        "                    )\n",
        "\n",
        "                return predicted_rating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9NUxC_8F4mDD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Uses scikit-learn K nearest neighbors Classifier\n",
        "\"\"\"\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class KNNClassifier:\n",
        "    \"\"\"\n",
        "    Uses scikit-learn K nearest neighbors Classifier to run experiments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        # Ran experiments and found that Accuracy does NOT change\n",
        "        # if algorithm or leaf_size is altered, thus choosing algorithm = auto (default)\n",
        "        self.classifiers = [\n",
        "            # Changing n_neighbors\n",
        "            KNeighborsClassifier(n_neighbors=3, weights=\"uniform\", p=2, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=5, weights=\"uniform\", p=2, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=7, weights=\"uniform\", p=2, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=9, weights=\"uniform\", p=2, n_jobs=-1),\n",
        "            # Changing weights, now closer neighbors have larger weights\n",
        "            KNeighborsClassifier(n_neighbors=3, weights=\"distance\", p=2, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=5, weights=\"distance\", p=2, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=7, weights=\"distance\", p=2, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=9, weights=\"distance\", p=2, n_jobs=-1),\n",
        "            # Changing l_p for minkowski distance\n",
        "            KNeighborsClassifier(n_neighbors=3, weights=\"distance\", p=3, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=5, weights=\"distance\", p=3, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=5, weights=\"distance\", p=4, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=7, weights=\"distance\", p=4, n_jobs=-1),\n",
        "            KNeighborsClassifier(n_neighbors=9, weights=\"distance\", p=4, n_jobs=-1),\n",
        "        ]\n",
        "\n",
        "        self.classifier = \"\"\n",
        "        for classifier in self.classifiers:\n",
        "            self.classifier = classifier\n",
        "            self.train(dataset)\n",
        "            self.test(dataset)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def test(self, dataset):\n",
        "        \"\"\"\n",
        "        Report Error Metrics on Test Data\n",
        "        \"\"\"\n",
        "        x_test = dataset[\"test\"][:, :-1]\n",
        "        y_test = dataset[\"test\"][:, -1]\n",
        "\n",
        "        y_pred = self.classifier.predict(x_test)\n",
        "\n",
        "        params = self.classifier.get_params()\n",
        "        print(\"K-Nearest Neighbors Parameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nAccuracy: \" + str(accuracy))\n",
        "        error_rate = round(1.0 - accuracy, 4)\n",
        "        print(\"Error rate: \" + str(error_rate))\n",
        "        print(\"-------------------------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qUKPYYeL4pr6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Uses scikit-learn SVM Classifier\n",
        "\"\"\"\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class SVMClassifier:\n",
        "    \"\"\"\n",
        "    Uses scikit-learnSVM Classifier to run experiments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        self.classifiers = [\n",
        "            # Changing Penalty Regularization\n",
        "            SVC(C=1.0, kernel=\"rbf\", gamma=\"scale\", random_state=1),\n",
        "            SVC(C=2.0, kernel=\"rbf\", gamma=\"scale\", random_state=1),\n",
        "            SVC(C=0.5, kernel=\"rbf\", gamma=\"scale\", random_state=1),\n",
        "            # Changing Kernel\n",
        "            SVC(C=1.0, kernel=\"sigmoid\", gamma=\"scale\", random_state=1),\n",
        "            SVC(C=2.0, kernel=\"sigmoid\", gamma=\"scale\", random_state=1),\n",
        "            SVC(C=1.0, kernel=\"linear\", gamma=\"scale\", random_state=1),\n",
        "            SVC(C=2.0, kernel=\"linear\", gamma=\"scale\", random_state=1),\n",
        "            # Using Poly Kernel with different degree values\n",
        "            SVC(C=1.0, kernel=\"poly\", degree=3, gamma=\"scale\", random_state=1),\n",
        "            SVC(C=1.0, kernel=\"poly\", degree=4, gamma=\"scale\", random_state=1),\n",
        "            SVC(C=1.0, kernel=\"poly\", degree=5, gamma=\"scale\", random_state=1),\n",
        "            # Changing Gamma\n",
        "            SVC(C=1.0, kernel=\"rbf\", gamma=\"auto\", random_state=1),\n",
        "            SVC(C=1.0, kernel=\"linear\", gamma=\"auto\", random_state=1),\n",
        "            SVC(C=1.0, kernel=\"poly\", gamma=\"auto\", random_state=1),\n",
        "            SVC(C=1.0, kernel=\"sigmoid\", gamma=\"auto\", random_state=1),\n",
        "        ]\n",
        "\n",
        "        self.classifier = \"\"\n",
        "        for classifier in self.classifiers:\n",
        "            self.classifier = classifier\n",
        "            self.train(dataset)\n",
        "            self.test(dataset)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def test(self, dataset):\n",
        "        \"\"\"\n",
        "        Report Error Metrics on Test Data\n",
        "        \"\"\"\n",
        "        x_test = dataset[\"test\"][:, :-1]\n",
        "        y_test = dataset[\"test\"][:, -1]\n",
        "\n",
        "        y_pred = self.classifier.predict(x_test)\n",
        "\n",
        "        params = self.classifier.get_params()\n",
        "        print(\"SVC Parameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nAccuracy: \" + str(accuracy))\n",
        "        error_rate = round(1.0 - accuracy, 4)\n",
        "        print(\"Error rate: \" + str(error_rate))\n",
        "        print(\"-------------------------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tf4zgrfe4ten"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Instantiates and Runs all models\n",
        "\"\"\"\n",
        "\n",
        "import zipfile\n",
        "import sys\n",
        "from os.path import exists\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "class AllModels:\n",
        "    \"\"\"\n",
        "    Instantiates and Runs all models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models=\"all\"):\n",
        "        self.netflix_dataset = {\n",
        "            \"movies\": {},\n",
        "            \"ratings\": {\n",
        "                \"train\": defaultdict(dict),\n",
        "                \"test\": defaultdict(dict),\n",
        "            },\n",
        "        }\n",
        "\n",
        "        if models == \"all\" or \"collab\" in models:\n",
        "            dataset_path = \"./netflix/\"\n",
        "            self.read_netflix_dataset(dataset_path)\n",
        "            print(\"Running Collaborative Filtering on Netflix Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            CollabFilter(self.netflix_dataset)\n",
        "            print(\"*************************************************\\n\")\n",
        "\n",
        "        mnist_dataset = defaultdict()\n",
        "        if models == \"all\" or \"svm\" in models or \"knn\" in models:\n",
        "            # Fetch MNIST dataset for SVM and KNN\n",
        "            x, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
        "            x = x / 255\n",
        "\n",
        "            x_train, x_test = x[:60000], x[60000:]\n",
        "            y_train, y_test = y[:60000], y[60000:]\n",
        "\n",
        "            mnist_dataset[\"train\"] = np.column_stack((x_train, y_train))\n",
        "            mnist_dataset[\"test\"] = np.column_stack((x_test, y_test))\n",
        "\n",
        "        if models == \"all\" or \"svm\" in models:\n",
        "            print(\"Running SVM Experiments on MNIST Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            SVMClassifier(mnist_dataset)\n",
        "            print(\"*************************************************\\n\")\n",
        "\n",
        "        if models == \"all\" or \"knn\" in models:\n",
        "            print(\"Running K Nearest Neighbors Experiments on MNIST Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            KNNClassifier(mnist_dataset)\n",
        "            print(\"*************************************************\\n\")\n",
        "\n",
        "    def read_netflix_dataset(self, dataset_path):\n",
        "        \"\"\"\n",
        "        Extracts and Reads txt files from the Netflix dataset\n",
        "        \"\"\"\n",
        "        if not exists(dataset_path):\n",
        "            with zipfile.ZipFile(\"./netflix.zip\", \"r\") as zip_ref:\n",
        "                zip_ref.extractall(dataset_path)\n",
        "\n",
        "        movies_dataset = dataset_path + \"movie_titles.txt\"\n",
        "        train_ratings = dataset_path + \"TrainingRatings.txt\"\n",
        "        test_ratings = dataset_path + \"TestingRatings.txt\"\n",
        "\n",
        "        with open(movies_dataset, \"r\", encoding=\"latin-1\") as mv:\n",
        "            lines = mv.readlines()\n",
        "            for line in lines:\n",
        "                mv_data = line.split(\",\")\n",
        "                mv_id = mv_data[0]\n",
        "                mv_year = mv_data[1]\n",
        "                mv_name = mv_data[2].strip()\n",
        "                self.netflix_dataset[\"movies\"][mv_id] = {\n",
        "                    \"year\": mv_year,\n",
        "                    \"name\": mv_name,\n",
        "                }\n",
        "\n",
        "        with open(train_ratings, \"r\", encoding=\"utf-8\") as trn:\n",
        "            lines = trn.readlines()\n",
        "            for line in lines:\n",
        "                trn_data = line.split(\",\")\n",
        "                mv_id = trn_data[0]\n",
        "                user_id = trn_data[1]\n",
        "                user_mv_rating = float(trn_data[2].strip())\n",
        "                self.netflix_dataset[\"ratings\"][\"train\"][user_id][\n",
        "                    mv_id\n",
        "                ] = user_mv_rating\n",
        "\n",
        "        with open(test_ratings, \"r\", encoding=\"utf-8\") as tst:\n",
        "            lines = tst.readlines()\n",
        "            for line in lines:\n",
        "                tst_data = line.split(\",\")\n",
        "                mv_id = tst_data[0]\n",
        "                user_id = tst_data[1]\n",
        "                user_mv_rating = float(tst_data[2].strip())\n",
        "                self.netflix_dataset[\"ratings\"][\"test\"][user_id][mv_id] = user_mv_rating\n",
        "\n",
        "\n",
        "if __name__:\n",
        "    # Run all models passed as arguments\n",
        "    if len(sys.argv) >= 2:\n",
        "        MODELS = \" \".join(sys.argv[1:])\n",
        "        all_models = AllModels(MODELS)\n",
        "    else:\n",
        "        print(\n",
        "            'Argument (\"all\" or \"collab\" or \"knn\" or \"svm\") needed.',\n",
        "            file=sys.stderr,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNSt-cpn43YM"
      },
      "source": [
        "Driver Code to run the above python scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrE9WSmb4_f6",
        "outputId": "8a27ec2c-8c4f-44f2-861d-91d00dc9bd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SVM Experiments on MNIST Dataset\n",
            "*************************************************\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: rbf\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9792\n",
            "Error rate: 0.0208\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 2.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: rbf\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9831\n",
            "Error rate: 0.0169\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 0.5\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: rbf\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9759\n",
            "Error rate: 0.0241\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: sigmoid\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.7759\n",
            "Error rate: 0.2241\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 2.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: sigmoid\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.7703\n",
            "Error rate: 0.2297\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: linear\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9404\n",
            "Error rate: 0.0596\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 2.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: linear\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9381\n",
            "Error rate: 0.0619\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: scale\n",
            "kernel: poly\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9771\n",
            "Error rate: 0.0229\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 4\n",
            "gamma: scale\n",
            "kernel: poly\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9698\n",
            "Error rate: 0.0302\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 5\n",
            "gamma: scale\n",
            "kernel: poly\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9561\n",
            "Error rate: 0.0439\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: auto\n",
            "kernel: rbf\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9446\n",
            "Error rate: 0.0554\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: auto\n",
            "kernel: linear\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9404\n",
            "Error rate: 0.0596\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: auto\n",
            "kernel: poly\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.5867\n",
            "Error rate: 0.4133\n",
            "-------------------------------------------------\n",
            "\n",
            "SVC Parameters:\n",
            "C: 1.0\n",
            "break_ties: False\n",
            "cache_size: 200\n",
            "class_weight: None\n",
            "coef0: 0.0\n",
            "decision_function_shape: ovr\n",
            "degree: 3\n",
            "gamma: auto\n",
            "kernel: sigmoid\n",
            "max_iter: -1\n",
            "probability: False\n",
            "random_state: 1\n",
            "shrinking: True\n",
            "tol: 0.001\n",
            "verbose: False\n",
            "\n",
            "Accuracy: 0.9315\n",
            "Error rate: 0.0685\n",
            "-------------------------------------------------\n",
            "\n",
            "*************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.AllModels at 0x7a6f4d6b70d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "AllModels(models=\"svm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "prIY1ufh6ig6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fcf153-ff09-49e8-dce9-3972249f5492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K Nearest Neighbors Experiments on MNIST Dataset\n",
            "*************************************************\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 3\n",
            "p: 2\n",
            "weights: uniform\n",
            "\n",
            "Accuracy: 0.9705\n",
            "Error rate: 0.0295\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 5\n",
            "p: 2\n",
            "weights: uniform\n",
            "\n",
            "Accuracy: 0.9688\n",
            "Error rate: 0.0312\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 7\n",
            "p: 2\n",
            "weights: uniform\n",
            "\n",
            "Accuracy: 0.9694\n",
            "Error rate: 0.0306\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 9\n",
            "p: 2\n",
            "weights: uniform\n",
            "\n",
            "Accuracy: 0.9659\n",
            "Error rate: 0.0341\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 3\n",
            "p: 2\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9717\n",
            "Error rate: 0.0283\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 5\n",
            "p: 2\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9691\n",
            "Error rate: 0.0309\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 7\n",
            "p: 2\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.97\n",
            "Error rate: 0.03\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 9\n",
            "p: 2\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9673\n",
            "Error rate: 0.0327\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 3\n",
            "p: 3\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9728\n",
            "Error rate: 0.0272\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 5\n",
            "p: 3\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9735\n",
            "Error rate: 0.0265\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 5\n",
            "p: 4\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9753\n",
            "Error rate: 0.0247\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 7\n",
            "p: 4\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9741\n",
            "Error rate: 0.0259\n",
            "-------------------------------------------------\n",
            "\n",
            "K-Nearest Neighbors Parameters:\n",
            "algorithm: auto\n",
            "leaf_size: 30\n",
            "metric: minkowski\n",
            "metric_params: None\n",
            "n_jobs: -1\n",
            "n_neighbors: 9\n",
            "p: 4\n",
            "weights: distance\n",
            "\n",
            "Accuracy: 0.9724\n",
            "Error rate: 0.0276\n",
            "-------------------------------------------------\n",
            "\n",
            "*************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.AllModels at 0x7ded25a03f70>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "AllModels(models=\"knn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPiT2fmG-Rrb"
      },
      "source": [
        "Not Running Collaborative Filtering on Colab because it takes a lot of time (9-10 hours) to complete. I've had problems in previous project with Colab when the jobs take more than 3-4 hours to run.\n",
        "\n",
        "I ran Collaborative Filtering on my laptop, and I've attached the results below.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Running Collaborative Filtering on Netflix Dataset\n",
        "*************************************************\n",
        "\n",
        "Mean absolute error: 0.7902364002686847\n",
        "\n",
        "Root mean squared error: 0.9887258652823789"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}