{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ViAdUOeMRVEX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Uses scikit-learn Bagging Ensemble Classifier\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BaggingClsfr:\n",
        "    \"\"\"\n",
        "    Uses scikit-learn Bagging Ensemble Classifier to run experiments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, compute_f1=True):\n",
        "        self.classifier = \"\"\n",
        "        self.dtree = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "        self.train(dataset)\n",
        "        self.tune_parameters(dataset, compute_f1)\n",
        "        self.train_after_tuning(dataset)\n",
        "        self.test(dataset, compute_f1)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        self.classifier = BaggingClassifier(\n",
        "            random_state=0,\n",
        "            estimator=self.dtree,\n",
        "        )\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def tune_parameters(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Tries multiple parameters and chooses the best set of parameters for the classifier\n",
        "        \"\"\"\n",
        "        classifiers = [\n",
        "            # change n_estimators\n",
        "            BaggingClassifier(random_state=0, estimator=self.dtree, n_estimators=15),\n",
        "            BaggingClassifier(random_state=0, estimator=self.dtree, n_estimators=20),\n",
        "            BaggingClassifier(random_state=0, estimator=self.dtree, n_estimators=30),\n",
        "            # change max_samples\n",
        "            BaggingClassifier(\n",
        "                random_state=0, estimator=self.dtree, n_estimators=15, max_samples=5\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0, estimator=self.dtree, n_estimators=20, max_samples=5\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0, estimator=self.dtree, n_estimators=20, max_samples=10\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0, estimator=self.dtree, n_estimators=30, max_samples=10\n",
        "            ),\n",
        "            # change max_features\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=15,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=30,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "            ),\n",
        "            # change oob_score\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=15,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "                oob_score=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "                oob_score=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "                oob_score=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=30,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "                oob_score=True,\n",
        "            ),\n",
        "            # change bootstrap_features\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=15,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "                oob_score=True,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "                oob_score=True,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "                oob_score=True,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=30,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "                oob_score=True,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=15,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "                oob_score=False,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=5,\n",
        "                max_features=5,\n",
        "                oob_score=False,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=20,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "                oob_score=False,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "            BaggingClassifier(\n",
        "                random_state=0,\n",
        "                estimator=self.dtree,\n",
        "                n_estimators=30,\n",
        "                max_samples=10,\n",
        "                max_features=10,\n",
        "                oob_score=False,\n",
        "                bootstrap_features=True,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        # Predict using the default classifier\n",
        "        y_pred = self.classifier.predict(x_valid)\n",
        "        best_accuracy = accuracy_score(y_valid, y_pred)\n",
        "        print(\"Tuning default accuracy: \" + str(best_accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            best_f1 = f1_score(y_valid, y_pred)\n",
        "            print(\"Tuning default f1 score: \" + str(best_f1))\n",
        "\n",
        "        for clsf in classifiers:\n",
        "            clsf.fit(x_train, y_train)\n",
        "            y_pred = clsf.predict(x_valid)\n",
        "\n",
        "            accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "            if compute_f1:\n",
        "                f1_scr = f1_score(y_valid, y_pred)\n",
        "\n",
        "            # If this classifier config is better, then choose it\n",
        "            if accuracy > best_accuracy:\n",
        "                if compute_f1:\n",
        "                    if f1_scr > best_f1:\n",
        "                        best_accuracy = accuracy\n",
        "                        best_f1 = f1_scr\n",
        "                        self.classifier = clsf\n",
        "                else:\n",
        "                    best_accuracy = accuracy\n",
        "                    self.classifier = clsf\n",
        "\n",
        "        print(\"Tuning best accuracy: \" + str(best_accuracy))\n",
        "        if compute_f1:\n",
        "            print(\"Tuning best F1 score: \" + str(best_f1))\n",
        "\n",
        "        params = self.classifier.get_params()\n",
        "        print(\"\\nBest Baggging Classifier Parameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    def train_after_tuning(self, dataset):\n",
        "        \"\"\"\n",
        "        Merge Train and Validation data and train the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        x_train = np.concatenate((x_train, x_valid), axis=0)\n",
        "        y_train = np.append(y_train, y_valid)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def test(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Report Accuracy & F1 score on Test Data\n",
        "        \"\"\"\n",
        "        x_test = dataset[\"test\"][:, :-1]\n",
        "        y_test = dataset[\"test\"][:, -1]\n",
        "\n",
        "        y_pred = self.classifier.predict(x_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nTest accuracy: \" + str(accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            f1_scr = f1_score(y_test, y_pred)\n",
        "            print(\"Test F1 score: \" + str(f1_scr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aSWxsORHRf5F"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Uses scikit-learn Decision Tree Classifier\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DecisionTreeClsfr:\n",
        "    \"\"\"\n",
        "    Uses scikit-learn Decision Tree Classifier to run experiments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, compute_f1=True):\n",
        "        self.classifier = \"\"\n",
        "\n",
        "        self.train(dataset)\n",
        "        self.tune_parameters(dataset, compute_f1)\n",
        "        self.train_after_tuning(dataset)\n",
        "        self.test(dataset, compute_f1)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains the classifier on training data\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        self.classifier = DecisionTreeClassifier(random_state=0)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def tune_parameters(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Tries multiple parameters and chooses the best set of parameters for the classifier\n",
        "        \"\"\"\n",
        "        classifiers = [\n",
        "            # change criterion\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0, criterion=\"log_loss\", splitter=\"best\"\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0, criterion=\"entropy\", splitter=\"best\"\n",
        "            ),\n",
        "            # change splitter\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0, criterion=\"log_loss\", splitter=\"random\"\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0, criterion=\"entropy\", splitter=\"random\"\n",
        "            ),\n",
        "            # change min_samples_split & min_samples_leaf\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                splitter=\"best\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                splitter=\"best\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                splitter=\"random\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                splitter=\"random\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                splitter=\"best\",\n",
        "                min_samples_split=5,\n",
        "                max_features=\"sqrt\",\n",
        "                min_samples_leaf=2,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                splitter=\"best\",\n",
        "                min_samples_split=10,\n",
        "                max_features=\"log2\",\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                splitter=\"random\",\n",
        "                min_samples_split=5,\n",
        "                max_features=\"sqrt\",\n",
        "                min_samples_leaf=2,\n",
        "            ),\n",
        "            DecisionTreeClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                splitter=\"random\",\n",
        "                min_samples_split=10,\n",
        "                max_features=\"log2\",\n",
        "                min_samples_leaf=3,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        # Predict using the default classifier\n",
        "        y_pred = self.classifier.predict(x_valid)\n",
        "        best_accuracy = accuracy_score(y_valid, y_pred)\n",
        "        print(\"Tuning default accuracy: \" + str(best_accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            best_f1 = f1_score(y_valid, y_pred)\n",
        "            print(\"Tuning default f1 score: \" + str(best_f1))\n",
        "\n",
        "        for clsf in classifiers:\n",
        "            clsf.fit(x_train, y_train)\n",
        "            y_pred = clsf.predict(x_valid)\n",
        "\n",
        "            accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "            if compute_f1:\n",
        "                f1_scr = f1_score(y_valid, y_pred)\n",
        "\n",
        "            # If this classifier config is better, then choose it\n",
        "            if accuracy > best_accuracy:\n",
        "                if compute_f1:\n",
        "                    if f1_scr > best_f1:\n",
        "                        best_accuracy = accuracy\n",
        "                        best_f1 = f1_scr\n",
        "                        self.classifier = clsf\n",
        "                else:\n",
        "                    best_accuracy = accuracy\n",
        "                    self.classifier = clsf\n",
        "\n",
        "        print(\"Tuning best accuracy: \" + str(best_accuracy))\n",
        "        if compute_f1:\n",
        "            print(\"Tuning best F1 score: \" + str(best_f1))\n",
        "\n",
        "        params = self.classifier.get_params()\n",
        "        print(\"\\nBest Decision Tree Classifier Parameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    def train_after_tuning(self, dataset):\n",
        "        \"\"\"\n",
        "        Merge Train and Validation data and Train the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        x_train = np.concatenate((x_train, x_valid), axis=0)\n",
        "        y_train = np.append(y_train, y_valid)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def test(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Report Accuracy & F1 score on Test Data\n",
        "        \"\"\"\n",
        "        x_test = dataset[\"test\"][:, :-1]\n",
        "        y_test = dataset[\"test\"][:, -1]\n",
        "\n",
        "        y_pred = self.classifier.predict(x_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nTest accuracy: \" + str(accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            f1_scr = f1_score(y_test, y_pred)\n",
        "            print(\"Test F1 score: \" + str(f1_scr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1FUd7x5HRgIJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Uses scikit-learn Gradient Boosting Classifier\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GradientBoostingClsfr:\n",
        "    \"\"\"\n",
        "    Uses scikit-learn Gradient Boosting Classifier to run experiments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, compute_f1=True, multi_class=False):\n",
        "        self.classifier = \"\"\n",
        "\n",
        "        self.train(dataset)\n",
        "        self.tune_parameters(dataset, compute_f1, multi_class)\n",
        "        self.train_after_tuning(dataset)\n",
        "        self.test(dataset, compute_f1)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains all classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        self.classifier = GradientBoostingClassifier(random_state=0)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def tune_parameters(self, dataset, compute_f1=True, multi_class=False):\n",
        "        \"\"\"\n",
        "        Tries multiple parameters and chooses the best set of parameters for all classifier\n",
        "        \"\"\"\n",
        "        classifiers = [\n",
        "            # change n_estimators\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0, loss=\"log_loss\", n_estimators=200\n",
        "            ),\n",
        "            # change criterion\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0, loss=\"log_loss\", criterion=\"squared_error\"\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"log_loss\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "            ),\n",
        "            # change learning rate\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"log_loss\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "                learning_rate=0.05,\n",
        "            ),\n",
        "            # change subsample\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"log_loss\",\n",
        "                criterion=\"squared_error\",\n",
        "                subsample=0.75,\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"log_loss\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "                subsample=0.5,\n",
        "            ),\n",
        "            # change min_samples_split\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"log_loss\",\n",
        "                criterion=\"squared_error\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=10,\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"log_loss\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=10,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        multi_class_classifiers = [\n",
        "            # change loss\n",
        "            GradientBoostingClassifier(random_state=0, loss=\"exponential\"),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0, loss=\"exponential\", n_estimators=200\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0, loss=\"exponential\", criterion=\"squared_error\"\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"exponential\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"exponential\",\n",
        "                criterion=\"squared_error\",\n",
        "                subsample=0.75,\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"exponential\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "                learning_rate=0.05,\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"exponential\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"exponential\",\n",
        "                criterion=\"squared_error\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            GradientBoostingClassifier(\n",
        "                random_state=0,\n",
        "                loss=\"exponential\",\n",
        "                n_estimators=200,\n",
        "                criterion=\"squared_error\",\n",
        "                subsample=0.5,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        if not multi_class:\n",
        "            classifiers = classifiers + multi_class_classifiers\n",
        "\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        # Predict using the default classifier\n",
        "        y_pred = self.classifier.predict(x_valid)\n",
        "        best_accuracy = accuracy_score(y_valid, y_pred)\n",
        "        print(\"Tuning default accuracy: \" + str(best_accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            best_f1 = f1_score(y_valid, y_pred)\n",
        "            print(\"Tuning default f1 score: \" + str(best_f1))\n",
        "\n",
        "        for clsf in classifiers:\n",
        "            clsf.fit(x_train, y_train)\n",
        "            y_pred = clsf.predict(x_valid)\n",
        "\n",
        "            accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "            if compute_f1:\n",
        "                f1_scr = f1_score(y_valid, y_pred)\n",
        "\n",
        "            # If this classifier config is better, then choose it\n",
        "            if accuracy > best_accuracy:\n",
        "                if compute_f1:\n",
        "                    if f1_scr > best_f1:\n",
        "                        best_accuracy = accuracy\n",
        "                        best_f1 = f1_scr\n",
        "                        self.classifier = clsf\n",
        "                else:\n",
        "                    best_accuracy = accuracy\n",
        "                    self.classifier = clsf\n",
        "\n",
        "        print(\"Tuning best accuracy: \" + str(best_accuracy))\n",
        "        if compute_f1:\n",
        "            print(\"Tuning best F1 score: \" + str(best_f1))\n",
        "\n",
        "        params = self.classifier.get_params()\n",
        "        print(\"\\nBest Gradient Boosting Classifier Parameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    def train_after_tuning(self, dataset):\n",
        "        \"\"\"\n",
        "        Merge Train and Validation data and Train the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        x_train = np.concatenate((x_train, x_valid), axis=0)\n",
        "        y_train = np.append(y_train, y_valid)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def test(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Report Accuracy & F1 score on Test Data\n",
        "        \"\"\"\n",
        "        x_test = dataset[\"test\"][:, :-1]\n",
        "        y_test = dataset[\"test\"][:, -1]\n",
        "\n",
        "        y_pred = self.classifier.predict(x_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nTest accuracy: \" + str(accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            f1_scr = f1_score(y_test, y_pred)\n",
        "            print(\"Test F1 score: \" + str(f1_scr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W4RoIwmlRgXB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Uses scikit-learn Random Forest Classifier\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RandomForestClsfr:\n",
        "    \"\"\"\n",
        "    Uses scikit-learn Random Forest Classifier to run experiments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, compute_f1=True):\n",
        "        self.classifier = \"\"\n",
        "\n",
        "        self.train(dataset)\n",
        "        self.tune_parameters(dataset, compute_f1)\n",
        "        self.train_after_tuning(dataset)\n",
        "        self.test(dataset, compute_f1)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        self.classifier = RandomForestClassifier(random_state=0)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def tune_parameters(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Tries multiple parameters and chooses the best set of parameters for the classifier\n",
        "        \"\"\"\n",
        "        classifiers = [\n",
        "            # change ecriterion\n",
        "            RandomForestClassifier(random_state=0, criterion=\"entropy\"),\n",
        "            RandomForestClassifier(random_state=0, criterion=\"log_loss\"),\n",
        "            # change min_samples_split\n",
        "            RandomForestClassifier(\n",
        "                random_state=0, criterion=\"entropy\", min_samples_split=5\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0, criterion=\"log_loss\", min_samples_split=5\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0, criterion=\"entropy\", min_samples_split=10\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0, criterion=\"log_loss\", min_samples_split=10\n",
        "            ),\n",
        "            # change min_samples_leaf\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=5,\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=10,\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=10,\n",
        "            ),\n",
        "            # change max_features\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=5,\n",
        "                max_features=\"log2\",\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=5,\n",
        "                max_features=\"log2\",\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"entropy\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=10,\n",
        "                max_features=\"log2\",\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                random_state=0,\n",
        "                criterion=\"log_loss\",\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=10,\n",
        "                max_features=\"log2\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "                # Predict using the default classifier\n",
        "        y_pred = self.classifier.predict(x_valid)\n",
        "        best_accuracy = accuracy_score(y_valid, y_pred)\n",
        "        print(\"Tuning default accuracy: \" + str(best_accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            best_f1 = f1_score(y_valid, y_pred)\n",
        "            print(\"Tuning default f1 score: \" + str(best_f1))\n",
        "\n",
        "        for clsf in classifiers:\n",
        "            clsf.fit(x_train, y_train)\n",
        "            y_pred = clsf.predict(x_valid)\n",
        "\n",
        "            accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "            if compute_f1:\n",
        "                f1_scr = f1_score(y_valid, y_pred)\n",
        "\n",
        "            # If this classifier config is better, then choose it\n",
        "            if accuracy > best_accuracy:\n",
        "                if compute_f1:\n",
        "                    if f1_scr > best_f1:\n",
        "                        best_accuracy = accuracy\n",
        "                        best_f1 = f1_scr\n",
        "                        self.classifier = clsf\n",
        "                else:\n",
        "                    best_accuracy = accuracy\n",
        "                    self.classifier = clsf\n",
        "\n",
        "        print(\"Tuning best accuracy: \" + str(best_accuracy))\n",
        "        if compute_f1:\n",
        "            print(\"Tuning best F1 score: \" + str(best_f1))\n",
        "\n",
        "        params = self.classifier.get_params()\n",
        "        print(\"\\nBest Random Forest Classifier Parameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    def train_after_tuning(self, dataset):\n",
        "        \"\"\"\n",
        "        Merge Train and Validation data and train the classifier\n",
        "        \"\"\"\n",
        "        x_train = dataset[\"train\"][:, :-1]\n",
        "        y_train = dataset[\"train\"][:, -1]\n",
        "        x_valid = dataset[\"valid\"][:, :-1]\n",
        "        y_valid = dataset[\"valid\"][:, -1]\n",
        "\n",
        "        x_train = np.concatenate((x_train, x_valid), axis=0)\n",
        "        y_train = np.append(y_train, y_valid)\n",
        "        self.classifier.fit(x_train, y_train)\n",
        "\n",
        "    def test(self, dataset, compute_f1=True):\n",
        "        \"\"\"\n",
        "        Report Accuracy & F1 score on Test Data\n",
        "        \"\"\"\n",
        "        x_test = dataset[\"test\"][:, :-1]\n",
        "        y_test = dataset[\"test\"][:, -1]\n",
        "\n",
        "        y_pred = self.classifier.predict(x_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nTest accuracy: \" + str(accuracy))\n",
        "\n",
        "        if compute_f1:\n",
        "            f1_scr = f1_score(y_test, y_pred)\n",
        "            print(\"Test F1 score: \" + str(f1_scr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zwVsVIFjWlCT"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def read_datasets(dataset_path=\"./all_data\", zip_file = \"./project2_data.zip\") -> defaultdict(lambda: defaultdict(dict)):\n",
        "        \"\"\"\n",
        "        Extracts and Reads csv files from the dataset\n",
        "        \"\"\"\n",
        "        dataset = defaultdict(lambda: defaultdict(dict))\n",
        "        if not exists(dataset_path):\n",
        "            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(\".\")\n",
        "\n",
        "        dataset_csv_files = listdir(dataset_path)\n",
        "        for csv_file in dataset_csv_files:\n",
        "            name_parts = csv_file.split(\"_\")\n",
        "            d_type = name_parts[0]\n",
        "            d_clauses = name_parts[1]\n",
        "            d_examples = name_parts[2].split(\".\")[0]\n",
        "            csv_file_path = join(dataset_path, csv_file)\n",
        "            dataset[d_clauses][d_examples][d_type] = np.genfromtxt(\n",
        "                csv_file_path, delimiter=\",\"\n",
        "            )\n",
        "\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "drsqtOyIR8Mi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Instantiates and Runs all models\n",
        "\"\"\"\n",
        "\n",
        "import zipfile\n",
        "import sys\n",
        "from os.path import exists, join\n",
        "from os import listdir\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class AllModels:\n",
        "    \"\"\"\n",
        "    Instantiates and Runs all models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, models=\"all\"):\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if models == \"all\" or \"dtree\" in models:\n",
        "            print(\"Running Decision Tree Classifier\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.decision_tree_classifiers = defaultdict(\n",
        "                lambda: defaultdict(lambda: DecisionTreeClsfr)\n",
        "            )\n",
        "\n",
        "            for n_clauses in self.dataset:\n",
        "                for n_examples in self.dataset[n_clauses]:\n",
        "                    print(n_clauses + \"->\" + n_examples)\n",
        "                    print(\"---------------------------------------------\")\n",
        "                    self.decision_tree_classifiers[n_clauses][\n",
        "                        n_examples\n",
        "                    ] = DecisionTreeClsfr(self.dataset[n_clauses][n_examples])\n",
        "                    print(\"---------------------------------------------\\n\")\n",
        "\n",
        "        if models == \"all\" or \"bagging\" in models:\n",
        "            print(\"Running Bagging Classifier\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.bagging_classifiers = defaultdict(\n",
        "                lambda: defaultdict(lambda: BaggingClsfr)\n",
        "            )\n",
        "\n",
        "            for n_clauses in self.dataset:\n",
        "                for n_examples in self.dataset[n_clauses]:\n",
        "                    print(n_clauses + \"->\" + n_examples)\n",
        "                    print(\"---------------------------------------------\")\n",
        "                    self.bagging_classifiers[n_clauses][n_examples] = BaggingClsfr(\n",
        "                        self.dataset[n_clauses][n_examples]\n",
        "                    )\n",
        "                    print(\"---------------------------------------------\\n\")\n",
        "\n",
        "        if models == \"all\" or \"randomforest\" in models:\n",
        "            print(\"Running Random Forest Classifier\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.random_forest_classifiers = defaultdict(\n",
        "                lambda: defaultdict(lambda: RandomForestClsfr)\n",
        "            )\n",
        "\n",
        "            for n_clauses in self.dataset:\n",
        "                for n_examples in self.dataset[n_clauses]:\n",
        "                    print(n_clauses + \"->\" + n_examples)\n",
        "                    print(\"---------------------------------------------\")\n",
        "                    print(n_clauses + \"->\" + n_examples)\n",
        "                    self.random_forest_classifiers[n_clauses][\n",
        "                        n_examples\n",
        "                    ] = RandomForestClsfr(self.dataset[n_clauses][n_examples])\n",
        "                    print(\"---------------------------------------------\\n\")\n",
        "\n",
        "        if models == \"all\" or \"gradientboost\" in models:\n",
        "            print(\"Running Gradient Boost Classifier\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.gradient_boosting_classifiers = defaultdict(\n",
        "                lambda: defaultdict(lambda: GradientBoostingClsfr)\n",
        "            )\n",
        "\n",
        "            for n_clauses in self.dataset:\n",
        "                for n_examples in self.dataset[n_clauses]:\n",
        "                    print(n_clauses + \"->\" + n_examples)\n",
        "                    print(\"---------------------------------------------\")\n",
        "                    self.gradient_boosting_classifiers[n_clauses][\n",
        "                        n_examples\n",
        "                    ] = GradientBoostingClsfr(self.dataset[n_clauses][n_examples])\n",
        "                    print(\"---------------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cXfb9UVHDh-k"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fetches MNIST dataset and runs all models on it\n",
        "\"\"\"\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "\n",
        "class MNISTExperiments:\n",
        "    \"\"\"\n",
        "    Fetches MNIST dataset and runs all models on it\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models=\"all\"):\n",
        "        x, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
        "        x = x / 255\n",
        "\n",
        "        x_train, x_valid, x_test = x[:40000], x[40000:60000], x[60000:]\n",
        "        y_train, y_valid, y_test = y[:40000], y[40000:60000], y[60000:]\n",
        "\n",
        "        dataset = {\n",
        "            \"train\": np.column_stack((x_train, y_train)),\n",
        "            \"valid\": np.column_stack((x_valid, y_valid)),\n",
        "            \"test\": np.column_stack((x_test, y_test)),\n",
        "        }\n",
        "\n",
        "        if models == \"all\" or \"dtree\" in models:\n",
        "            print(\"Running Decision Tree Classifier on MNIST Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.decision_tree_classifier = DecisionTreeClsfr(dataset, compute_f1=False)\n",
        "            print(\"---------------------------------------------\")\n",
        "\n",
        "        if models == \"all\" or \"bagging\" in models:\n",
        "            print(\"Running Bagging Classifier on MNIST Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.bagging_classifier = BaggingClsfr(dataset, compute_f1=False)\n",
        "            print(\"---------------------------------------------\")\n",
        "\n",
        "        if models == \"all\" or \"randomforest\" in models:\n",
        "            print(\"Running Random Forest Classifier on MNIST Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.random_forest_classifier = RandomForestClsfr(dataset, compute_f1=False)\n",
        "            print(\"---------------------------------------------\")\n",
        "\n",
        "        if models == \"all\" or \"gradientboost\" in models:\n",
        "            print(\"Running Gradient Boost Classifier on MNIST Dataset\")\n",
        "            print(\"*************************************************\\n\")\n",
        "            self.gradient_boosting_classifier = GradientBoostingClsfr(\n",
        "                dataset, compute_f1=False, multi_class=True\n",
        "            )\n",
        "            print(\"---------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Adl4FJaHLhY"
      },
      "source": [
        "Use driver classes to run models in datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y91As6guXfka"
      },
      "outputs": [],
      "source": [
        "project2_dataset = read_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKVbCuDmTaaz",
        "outputId": "c8c0c62c-a906-4295-cccf-cf52cf710dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Decision Tree Classifier\n",
            "*************************************************\n",
            "\n",
            "c300->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.7224\n",
            "Tuning default f1 score: 0.7209489344591878\n",
            "Tuning best accuracy: 0.7258\n",
            "Tuning best F1 score: 0.728191911181602\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.7588\n",
            "Test F1 score: 0.755771567436209\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.6145\n",
            "Tuning default f1 score: 0.613920881321983\n",
            "Tuning best accuracy: 0.6345\n",
            "Tuning best F1 score: 0.6206538661131291\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: log_loss\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 2\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.668\n",
            "Test F1 score: 0.6619144602851323\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.6\n",
            "Tuning default f1 score: 0.6330275229357798\n",
            "Tuning best accuracy: 0.605\n",
            "Tuning best F1 score: 0.6359447004608294\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: random\n",
            "\n",
            "Test accuracy: 0.655\n",
            "Test F1 score: 0.6877828054298643\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.7685\n",
            "Tuning default f1 score: 0.7735941320293399\n",
            "Tuning best accuracy: 0.809\n",
            "Tuning best F1 score: 0.8074596774193549\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.786\n",
            "Test F1 score: 0.7930367504835589\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.825\n",
            "Tuning default f1 score: 0.8278914240755311\n",
            "Tuning best accuracy: 0.8381\n",
            "Tuning best F1 score: 0.8367120524457892\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: log_loss\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 2\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.8479\n",
            "Test F1 score: 0.8480974732847298\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.73\n",
            "Tuning default f1 score: 0.7352941176470588\n",
            "Tuning best accuracy: 0.8\n",
            "Tuning best F1 score: 0.8000000000000002\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: log_loss\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 2\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: random\n",
            "\n",
            "Test accuracy: 0.7\n",
            "Test F1 score: 0.7087378640776699\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.962\n",
            "Tuning default f1 score: 0.9623389494549057\n",
            "Tuning best accuracy: 0.9695\n",
            "Tuning best F1 score: 0.9694235588972432\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: log_loss\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.973\n",
            "Test F1 score: 0.9733201581027668\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.94\n",
            "Tuning default f1 score: 0.9423076923076924\n",
            "Tuning best accuracy: 0.98\n",
            "Tuning best F1 score: 0.9801980198019802\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.95\n",
            "Test F1 score: 0.9509803921568627\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9784\n",
            "Tuning default f1 score: 0.9786350148367953\n",
            "Tuning best accuracy: 0.9844\n",
            "Tuning best F1 score: 0.9844342446617441\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: log_loss\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: random\n",
            "\n",
            "Test accuracy: 0.9837\n",
            "Test F1 score: 0.9838213399503721\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.82\n",
            "Tuning default f1 score: 0.8285714285714286\n",
            "Tuning best accuracy: 0.875\n",
            "Tuning best F1 score: 0.8743718592964824\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.885\n",
            "Test F1 score: 0.8878048780487805\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9348\n",
            "Tuning default f1 score: 0.9359276729559748\n",
            "Tuning best accuracy: 0.9475\n",
            "Tuning best F1 score: 0.9475890985324947\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: log_loss\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 2\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.9565\n",
            "Test F1 score: 0.9565824932628008\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.893\n",
            "Tuning default f1 score: 0.8959143968871596\n",
            "Tuning best accuracy: 0.9135\n",
            "Tuning best F1 score: 0.91431401684002\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.9085\n",
            "Test F1 score: 0.9116368903911153\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.7285\n",
            "Tuning default f1 score: 0.7271630991860114\n",
            "Tuning best accuracy: 0.7481\n",
            "Tuning best F1 score: 0.7530150014707324\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.7828\n",
            "Test F1 score: 0.78482266693085\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.661\n",
            "Tuning default f1 score: 0.661\n",
            "Tuning best accuracy: 0.692\n",
            "Tuning best F1 score: 0.691382765531062\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: random\n",
            "\n",
            "Test accuracy: 0.685\n",
            "Test F1 score: 0.6792260692464359\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.6\n",
            "Tuning default f1 score: 0.6116504854368932\n",
            "Tuning best accuracy: 0.67\n",
            "Tuning best F1 score: 0.6597938144329897\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.65\n",
            "Test F1 score: 0.6666666666666666\n",
            "---------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dtree = AllModels(models=\"dtree\", dataset=project2_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxJTHLywVDp5",
        "outputId": "3f28140f-3570-4b01-ec19-0dc35204dd11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Bagging Classifier\n",
            "*************************************************\n",
            "\n",
            "c300->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.8226\n",
            "Tuning default f1 score: 0.8168868703550785\n",
            "Tuning best accuracy: 0.8775\n",
            "Tuning best F1 score: 0.8813099505861833\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9081\n",
            "Test F1 score: 0.9109582404805736\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.748\n",
            "Tuning default f1 score: 0.7278617710583154\n",
            "Tuning best accuracy: 0.822\n",
            "Tuning best F1 score: 0.8200202224469162\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.8405\n",
            "Test F1 score: 0.8378240976105744\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.59\n",
            "Tuning default f1 score: 0.5494505494505494\n",
            "Tuning best accuracy: 0.63\n",
            "Tuning best F1 score: 0.6336633663366336\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 15\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.675\n",
            "Test F1 score: 0.6733668341708543\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.894\n",
            "Tuning default f1 score: 0.890495867768595\n",
            "Tuning best accuracy: 0.92\n",
            "Tuning best F1 score: 0.9198396793587175\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.932\n",
            "Test F1 score: 0.9327398615232443\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9278\n",
            "Tuning default f1 score: 0.9265065146579805\n",
            "Tuning best accuracy: 0.9481\n",
            "Tuning best F1 score: 0.9479490522515295\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9555\n",
            "Test F1 score: 0.9553705746665331\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.82\n",
            "Tuning default f1 score: 0.797752808988764\n",
            "Tuning best accuracy: 0.835\n",
            "Tuning best F1 score: 0.8253968253968254\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 15\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.875\n",
            "Test F1 score: 0.8756218905472637\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9875\n",
            "Tuning default f1 score: 0.9873673572511369\n",
            "Tuning best accuracy: 0.9895\n",
            "Tuning best F1 score: 0.9894099848714069\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 20\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.995\n",
            "Test F1 score: 0.9949899799599199\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.98\n",
            "Tuning default f1 score: 0.98\n",
            "Tuning best accuracy: 0.995\n",
            "Tuning best F1 score: 0.9950248756218906\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 20\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.97\n",
            "Test F1 score: 0.9696969696969697\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9946\n",
            "Tuning default f1 score: 0.9945870088211708\n",
            "Tuning best accuracy: 0.9961\n",
            "Tuning best F1 score: 0.9960941412118177\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.997\n",
            "Test F1 score: 0.9969963956748099\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.97\n",
            "Tuning default f1 score: 0.9705882352941176\n",
            "Tuning best accuracy: 0.97\n",
            "Tuning best F1 score: 0.9705882352941176\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 10\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.965\n",
            "Test F1 score: 0.9644670050761421\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9826\n",
            "Tuning default f1 score: 0.9824949698189136\n",
            "Tuning best accuracy: 0.9873\n",
            "Tuning best F1 score: 0.9872707226621229\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9887\n",
            "Test F1 score: 0.9886807572873886\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9655\n",
            "Tuning default f1 score: 0.9652392947103274\n",
            "Tuning best accuracy: 0.978\n",
            "Tuning best F1 score: 0.9778894472361808\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 20\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9815\n",
            "Test F1 score: 0.9813789632611978\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.8392\n",
            "Tuning default f1 score: 0.8323253388946819\n",
            "Tuning best accuracy: 0.8894\n",
            "Tuning best F1 score: 0.8878523626039344\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9216\n",
            "Test F1 score: 0.9216939672393129\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.776\n",
            "Tuning default f1 score: 0.7639620653319282\n",
            "Tuning best accuracy: 0.848\n",
            "Tuning best F1 score: 0.8467741935483871\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.8725\n",
            "Test F1 score: 0.8717948717948719\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.695\n",
            "Tuning default f1 score: 0.6666666666666666\n",
            "Tuning best accuracy: 0.765\n",
            "Tuning best F1 score: 0.7486631016042781\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.82\n",
            "Test F1 score: 0.82\n",
            "---------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bagging = AllModels(models=\"bagging\", dataset=project2_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTsgh3qsX5lO",
        "outputId": "f1590dc0-fe6f-4777-8907-61aa1b51b78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Random Forest Classifier\n",
            "*************************************************\n",
            "\n",
            "c300->d5000\n",
            "---------------------------------------------\n",
            "c300->d5000\n",
            "Tuning default accuracy: 0.8711\n",
            "Tuning default f1 score: 0.8715751718641027\n",
            "Tuning best accuracy: 0.8999\n",
            "Tuning best F1 score: 0.9017953497498282\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9087\n",
            "Test F1 score: 0.9105866222701009\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d1000\n",
            "---------------------------------------------\n",
            "c300->d1000\n",
            "Tuning default accuracy: 0.8465\n",
            "Tuning default f1 score: 0.8492881688757977\n",
            "Tuning best accuracy: 0.8625\n",
            "Tuning best F1 score: 0.8661800486618004\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 10\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.8765\n",
            "Test F1 score: 0.8780246913580246\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d100\n",
            "---------------------------------------------\n",
            "c300->d100\n",
            "Tuning default accuracy: 0.71\n",
            "Tuning default f1 score: 0.7010309278350516\n",
            "Tuning best accuracy: 0.75\n",
            "Tuning best F1 score: 0.7572815533980584\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.84\n",
            "Test F1 score: 0.8446601941747572\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d1000\n",
            "---------------------------------------------\n",
            "c1000->d1000\n",
            "Tuning default accuracy: 0.9865\n",
            "Tuning default f1 score: 0.9865336658354115\n",
            "Tuning best accuracy: 0.9965\n",
            "Tuning best F1 score: 0.9965052421367948\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: log2\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 10\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.994\n",
            "Test F1 score: 0.994005994005994\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d5000\n",
            "---------------------------------------------\n",
            "c1000->d5000\n",
            "Tuning default accuracy: 0.9927\n",
            "Tuning default f1 score: 0.992705106425502\n",
            "Tuning best accuracy: 0.9975\n",
            "Tuning best F1 score: 0.9975017487758568\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: log2\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9965\n",
            "Test F1 score: 0.9965038457696533\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d100\n",
            "---------------------------------------------\n",
            "c1000->d100\n",
            "Tuning default accuracy: 0.95\n",
            "Tuning default f1 score: 0.9494949494949495\n",
            "Tuning best accuracy: 0.98\n",
            "Tuning best F1 score: 0.98\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: log2\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.965\n",
            "Test F1 score: 0.964824120603015\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d1000\n",
            "---------------------------------------------\n",
            "c1800->d1000\n",
            "Tuning default accuracy: 1.0\n",
            "Tuning default f1 score: 1.0\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: gini\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d100\n",
            "---------------------------------------------\n",
            "c1800->d100\n",
            "Tuning default accuracy: 1.0\n",
            "Tuning default f1 score: 1.0\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: gini\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.995\n",
            "Test F1 score: 0.9949748743718593\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d5000\n",
            "---------------------------------------------\n",
            "c1800->d5000\n",
            "Tuning default accuracy: 0.9998\n",
            "Tuning default f1 score: 0.9997999599919984\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d100\n",
            "---------------------------------------------\n",
            "c1500->d100\n",
            "Tuning default accuracy: 1.0\n",
            "Tuning default f1 score: 1.0\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: gini\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d5000\n",
            "---------------------------------------------\n",
            "c1500->d5000\n",
            "Tuning default accuracy: 0.9996\n",
            "Tuning default f1 score: 0.9995999199839969\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: log2\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 10\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9998\n",
            "Test F1 score: 0.9997999599919984\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d1000\n",
            "---------------------------------------------\n",
            "c1500->d1000\n",
            "Tuning default accuracy: 0.9995\n",
            "Tuning default f1 score: 0.9995002498750626\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9995\n",
            "Test F1 score: 0.9994997498749374\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d5000\n",
            "---------------------------------------------\n",
            "c500->d5000\n",
            "Tuning default accuracy: 0.9431\n",
            "Tuning default f1 score: 0.9431170648805358\n",
            "Tuning best accuracy: 0.9545\n",
            "Tuning best F1 score: 0.9549816958543583\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: log2\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 10\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9559\n",
            "Test F1 score: 0.9563236604932158\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d1000\n",
            "---------------------------------------------\n",
            "c500->d1000\n",
            "Tuning default accuracy: 0.9025\n",
            "Tuning default f1 score: 0.9038935436175456\n",
            "Tuning best accuracy: 0.931\n",
            "Tuning best F1 score: 0.9320866141732284\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: log2\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 10\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.947\n",
            "Test F1 score: 0.947420634920635\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d100\n",
            "---------------------------------------------\n",
            "c500->d100\n",
            "Tuning default accuracy: 0.835\n",
            "Tuning default f1 score: 0.83248730964467\n",
            "Tuning best accuracy: 0.865\n",
            "Tuning best F1 score: 0.8682926829268293\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 5\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.87\n",
            "Test F1 score: 0.87\n",
            "---------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "randomforest = AllModels(models=\"randomforest\", dataset=project2_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpSiSEPzX8n0",
        "outputId": "5b59f262-3028-4ed3-9e74-4af4f67a2a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradient Boost Classifier\n",
            "*************************************************\n",
            "\n",
            "c300->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9793\n",
            "Tuning default f1 score: 0.9796919454527617\n",
            "Tuning best accuracy: 0.9927\n",
            "Tuning best F1 score: 0.9927529038022436\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.994\n",
            "Test F1 score: 0.9940357852882703\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9525\n",
            "Tuning default f1 score: 0.954039671020803\n",
            "Tuning best accuracy: 0.9745\n",
            "Tuning best F1 score: 0.975085490962384\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.991\n",
            "Test F1 score: 0.9910802775024777\n",
            "---------------------------------------------\n",
            "\n",
            "c300->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.72\n",
            "Tuning default f1 score: 0.7142857142857142\n",
            "Tuning best accuracy: 0.745\n",
            "Tuning best F1 score: 0.7411167512690355\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: exponential\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.835\n",
            "Test F1 score: 0.8374384236453202\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9865\n",
            "Tuning default f1 score: 0.9865738438587768\n",
            "Tuning best accuracy: 0.997\n",
            "Tuning best F1 score: 0.997002997002997\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: squared_error\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 0.5\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9975\n",
            "Test F1 score: 0.9975062344139651\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9976\n",
            "Tuning default f1 score: 0.9976028765481422\n",
            "Tuning best accuracy: 0.9996\n",
            "Tuning best F1 score: 0.9996001599360256\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9996\n",
            "Test F1 score: 0.9996001599360256\n",
            "---------------------------------------------\n",
            "\n",
            "c1000->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.91\n",
            "Tuning default f1 score: 0.9108910891089109\n",
            "Tuning best accuracy: 0.975\n",
            "Tuning best F1 score: 0.9751243781094527\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: squared_error\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: exponential\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 0.5\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.985\n",
            "Test F1 score: 0.9850746268656716\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 1.0\n",
            "Tuning default f1 score: 1.0\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 1.0\n",
            "Tuning default f1 score: 1.0\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.99\n",
            "Test F1 score: 0.99\n",
            "---------------------------------------------\n",
            "\n",
            "c1800->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 1.0\n",
            "Tuning default f1 score: 1.0\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9999\n",
            "Test F1 score: 0.9999000099990001\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.975\n",
            "Tuning default f1 score: 0.975609756097561\n",
            "Tuning best accuracy: 0.995\n",
            "Tuning best F1 score: 0.9950248756218906\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: squared_error\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 0.5\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9997\n",
            "Tuning default f1 score: 0.9997000299970004\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c1500->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9985\n",
            "Tuning default f1 score: 0.9985022466300548\n",
            "Tuning best accuracy: 1.0\n",
            "Tuning best F1 score: 1.0\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 1.0\n",
            "Test F1 score: 1.0\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d5000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.9811\n",
            "Tuning default f1 score: 0.9813811447148064\n",
            "Tuning best accuracy: 0.9962\n",
            "Tuning best F1 score: 0.9962136309286568\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9969\n",
            "Test F1 score: 0.9969089640043873\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d1000\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.968\n",
            "Tuning default f1 score: 0.9684729064039409\n",
            "Tuning best accuracy: 0.9835\n",
            "Tuning best F1 score: 0.9836552748885585\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: friedman_mse\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.996\n",
            "Test F1 score: 0.9960159362549801\n",
            "---------------------------------------------\n",
            "\n",
            "c500->d100\n",
            "---------------------------------------------\n",
            "Tuning default accuracy: 0.79\n",
            "Tuning default f1 score: 0.79\n",
            "Tuning best accuracy: 0.865\n",
            "Tuning best F1 score: 0.864321608040201\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: squared_error\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 0.5\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9\n",
            "Test F1 score: 0.9019607843137256\n",
            "---------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gradientboost = AllModels(models=\"gradientboost\", dataset=project2_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFAmujANDutx",
        "outputId": "172270df-d197-4eb8-97bc-10598a802f0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Decision Tree Classifier on MNIST Dataset\n",
            "*************************************************\n",
            "\n",
            "Tuning default accuracy: 0.86375\n",
            "Tuning best accuracy: 0.87645\n",
            "\n",
            "Best Decision Tree Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 5\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "random_state: 0\n",
            "splitter: best\n",
            "\n",
            "Test accuracy: 0.8875\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mnist_dtree = MNISTExperiments(\"dtree\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXyUNOYMDuiq",
        "outputId": "9e19d695-7565-419c-d6aa-ce09668828dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Bagging Classifier on MNIST Dataset\n",
            "*************************************************\n",
            "\n",
            "Tuning default accuracy: 0.93575\n",
            "Tuning best accuracy: 0.94955\n",
            "\n",
            "Best Baggging Classifier Parameters:\n",
            "base_estimator: deprecated\n",
            "bootstrap: True\n",
            "bootstrap_features: False\n",
            "estimator__ccp_alpha: 0.0\n",
            "estimator__class_weight: None\n",
            "estimator__criterion: gini\n",
            "estimator__max_depth: None\n",
            "estimator__max_features: None\n",
            "estimator__max_leaf_nodes: None\n",
            "estimator__min_impurity_decrease: 0.0\n",
            "estimator__min_samples_leaf: 1\n",
            "estimator__min_samples_split: 2\n",
            "estimator__min_weight_fraction_leaf: 0.0\n",
            "estimator__random_state: 0\n",
            "estimator__splitter: best\n",
            "estimator: DecisionTreeClassifier(random_state=0)\n",
            "max_features: 1.0\n",
            "max_samples: 1.0\n",
            "n_estimators: 30\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9554\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mnist_bagging = MNISTExperiments(\"bagging\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfnSggZzDua2",
        "outputId": "f2f5e780-b2a9-467d-8e38-530c10817895"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Random Forest Classifier on MNIST Dataset\n",
            "*************************************************\n",
            "\n",
            "Tuning default accuracy: 0.96475\n",
            "Tuning best accuracy: 0.9655\n",
            "\n",
            "Best Random Forest Classifier Parameters:\n",
            "bootstrap: True\n",
            "ccp_alpha: 0.0\n",
            "class_weight: None\n",
            "criterion: entropy\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "max_leaf_nodes: None\n",
            "max_samples: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 100\n",
            "n_jobs: None\n",
            "oob_score: False\n",
            "random_state: 0\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9692\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mnist_randomforest = MNISTExperiments(\"randomforest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rJIkm7kDuSo",
        "outputId": "f666f4f6-8d4f-4aaf-e81f-9dffcde26679"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradient Boost Classifier on MNIST Dataset\n",
            "*************************************************\n",
            "\n",
            "Tuning default accuracy: 0.94665\n",
            "Tuning best accuracy: 0.96045\n",
            "\n",
            "Best Gradient Boosting Classifier Parameters:\n",
            "ccp_alpha: 0.0\n",
            "criterion: squared_error\n",
            "init: None\n",
            "learning_rate: 0.1\n",
            "loss: log_loss\n",
            "max_depth: 3\n",
            "max_features: None\n",
            "max_leaf_nodes: None\n",
            "min_impurity_decrease: 0.0\n",
            "min_samples_leaf: 10\n",
            "min_samples_split: 10\n",
            "min_weight_fraction_leaf: 0.0\n",
            "n_estimators: 200\n",
            "n_iter_no_change: None\n",
            "random_state: 0\n",
            "subsample: 1.0\n",
            "tol: 0.0001\n",
            "validation_fraction: 0.1\n",
            "verbose: 0\n",
            "warm_start: False\n",
            "\n",
            "Test accuracy: 0.9619\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mnist_gradientboost = MNISTExperiments(\"gradientboost\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
